{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, balanced_accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from utils import preprocessing\n",
    "from imblearn.over_sampling import SMOTE, KMeansSMOTE\n",
    "from models import train\n",
    "# import pyswarms as ps\n",
    "# from pyswarms.discrete.binary import BinaryPSO\n",
    "import matplotlib.pyplot as plt\n",
    "# from pyswarms.utils.plotters import plot_contour, plot_surface\n",
    "# from pyswarms.utils.plotters.formatters import Designer\n",
    "# from pyswarms.utils.plotters.formatters import Mesher\n",
    "# import pygad\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler, CmaEsSampler\n",
    "from optuna.visualization import plot_contour\n",
    "from optuna.visualization import plot_edf\n",
    "from optuna.visualization import plot_intermediate_values\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_param_importances\n",
    "from optuna.visualization import plot_slice\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "np.random.seed(1066)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_multiclass_no_train_test(model, X_train, X_test, y_train, y_test, method):\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    # print('ACCURACY:', round(accuracy_score(y_test, pred), 3))\n",
    "    # print('BALANCED ACCURACY', round(balanced_accuracy_score(y_test, pred), 3))\n",
    "    # print('F-SCORE:', round(f1_score(y_test, pred, average='macro'), 3))\n",
    "    # print('PRECISION:', round(precision_score(y_test, pred, average='macro'), 3))\n",
    "    # print('RECALL:', round(recall_score(y_test, pred, average='macro'), 3))\n",
    "\n",
    "    df_metrics = pd.DataFrame(index=['ACCURACY', 'BALANCED ACCURACY', \n",
    "        'F-SCORE', 'PRECISION', 'RECALL'], data={method: [round(accuracy_score(y_test, pred), 3), \n",
    "                                                        round(balanced_accuracy_score(y_test, pred), 3),\n",
    "                                                        round(f1_score(y_test, pred, average='macro'), 3),\n",
    "                                                        round(precision_score(y_test, pred, average='macro'), 3),\n",
    "                                                        round(recall_score(y_test, pred, average='macro'), 3),\n",
    "                                                        ]})\n",
    "    return model, df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ACK    296\n",
       "CBC    158\n",
       "SEK    107\n",
       "NEV     41\n",
       "CEC     38\n",
       "MEL      6\n",
       "Name: Classe, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accepted_class = ['SEK', 'ACK', 'NEV', 'CBC', 'CEC', 'MEL']\n",
    "# df = pd.read_excel('espectros_multiclass.xlsx')\n",
    "# df = df[df['Classe'].isin(accepted_class)].drop(columns=['N', 'Proj', 'Sample'])\n",
    "# df['y'] = LabelEncoder().fit_transform(df['Classe'])\n",
    "# print(df['Classe'].value_counts())\n",
    "# df_train, df_test = train_test_split(df, test_size=0.30, random_state=42, stratify=df['y'])\n",
    "# print(df_train['Classe'].value_counts())\n",
    "# print(df_test['Classe'].value_counts())\n",
    "\n",
    "# df_train.reset_index(drop=True).to_excel('multilabel_data/treinamento-raw-multiclasse.xlsx', index=False)\n",
    "# df_test.reset_index(drop=True).to_excel('multilabel_data/teste-multiclasse.xlsx', index=False)\n",
    "\n",
    "# # SMOTE\n",
    "# sm = SMOTE(random_state=1066, k_neighbors=3)\n",
    "# X_smote, y_smote = sm.fit_resample(df_train.drop(columns=['Classe', 'y']), df_train['y'])\n",
    "# df_smote = pd.concat([pd.DataFrame(X_smote), pd.DataFrame(y_smote)], axis=1)\n",
    "# df_smote.to_excel('multilabel_data/treinamento-smote-raw-multiclasse.xlsx', index=False)\n",
    "\n",
    "# X_train = df_train.drop(columns=['Classe', 'y'])\n",
    "# y_train = df_train['y']\n",
    "# X_test = df_test.drop(columns=['Classe', 'y'])\n",
    "# y_test = df_test['y']\n",
    "\n",
    "# X_snv_train = pd.DataFrame(preprocessing.SNV(X_train), columns=X_train.columns)\n",
    "# X_snv_test = pd.DataFrame(preprocessing.SNV(X_test), columns=X_test.columns)\n",
    "\n",
    "# # GENERATE FEATURES DATASET\n",
    "# X_features_train = preprocessing.create_features(\n",
    "#         X_snv_train, n_subsets=26, mode='stats')\n",
    "\n",
    "# df_features_train = pd.concat([X_features_train, y_train.reset_index(drop=True)], axis=1)\n",
    "# df_features_train.to_excel('multilabel_data/treinamento-raw-features-multiclass.xlsx')\n",
    "\n",
    "# X_features_test = preprocessing.create_features(\n",
    "#         X_snv_test, n_subsets=26, mode='stats')\n",
    "\n",
    "# df_features_test = pd.concat([X_features_test, y_test.reset_index(drop=True)], axis=1)\n",
    "# df_features_test.to_excel('multilabel_data/teste-features-multiclass.xlsx')\n",
    "\n",
    "# X_snv_train = pd.DataFrame(preprocessing.SNV(X_smote), columns=X_smote.columns)\n",
    "\n",
    "# # GENERATE FEATURES DATASET\n",
    "# X_features_train = preprocessing.create_features(\n",
    "#         X_snv_train, n_subsets=26, mode='stats')\n",
    "\n",
    "# df_features_train = pd.concat([X_features_train, y_smote.reset_index(drop=True)], axis=1)\n",
    "# df_features_train.to_excel('multilabel_data/treinamento-smote-features-multiclass.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No preprocess\n",
      "X:  (452, 125) (194, 125)\n",
      "y:  (452,) (194,)\n",
      "SNV\n",
      "X:  (452, 125) (194, 125)\n",
      "Features\n",
      "(452, 312) (194, 312)\n"
     ]
    }
   ],
   "source": [
    "# RAW DATA\n",
    "df_train = pd.read_excel('multilabel_data/treinamento-raw-multiclasse.xlsx')\n",
    "df_test = pd.read_excel('multilabel_data/teste-multiclasse.xlsx')\n",
    "\n",
    "X_train = df_train.drop(columns=['Classe', 'y'])\n",
    "y_train = df_train['y']\n",
    "X_test = df_test.drop(columns=['Classe', 'y'])\n",
    "y_test = df_test['y']\n",
    "\n",
    "X_snv_train = pd.DataFrame(preprocessing.SNV(X_train), columns=X_train.columns)\n",
    "X_snv_test = pd.DataFrame(preprocessing.SNV(X_test), columns=X_test.columns)\n",
    "\n",
    "X_features_train = pd.read_excel('multilabel_data/treinamento-raw-features-multiclass.xlsx', index_col=0).drop(columns=['y'])\n",
    "X_features_test = pd.read_excel('multilabel_data/teste-features-multiclass.xlsx', index_col=0).drop(columns=['y'])\n",
    "\n",
    "print('No preprocess')\n",
    "print(\"X: \", X_train.shape, X_test.shape)\n",
    "print(\"y: \", y_train.shape, y_test.shape)\n",
    "\n",
    "print('SNV')\n",
    "print(\"X: \", X_snv_train.shape, X_snv_test.shape)\n",
    "\n",
    "print('Features')\n",
    "print(X_features_train.shape, X_features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No preprocess\n",
      "(1242, 125) (194, 125)\n",
      "SNV\n",
      "(1242, 125) (194, 125)\n",
      "Features\n",
      "(1242, 312) (194, 312)\n"
     ]
    }
   ],
   "source": [
    "# SMOTE\n",
    "\n",
    "df_train = pd.read_excel('multilabel_data/treinamento-smote-raw-multiclasse.xlsx')\n",
    "df_test = pd.read_excel('multilabel_data/teste-multiclasse.xlsx')\n",
    "\n",
    "X_train = df_train.drop(columns=['y'])\n",
    "y_train = df_train['y']\n",
    "X_test = df_test.drop(columns=['Classe', 'y'])\n",
    "y_test = df_test['y']\n",
    "\n",
    "X_snv_train = pd.DataFrame(preprocessing.SNV(X_train), columns=X_train.columns)\n",
    "X_snv_test = pd.DataFrame(preprocessing.SNV(X_test), columns=X_test.columns)\n",
    "\n",
    "X_features_train = pd.read_excel('multilabel_data/treinamento-smote-features-multiclass.xlsx', index_col=0).drop(columns=['y'])\n",
    "X_features_test = pd.read_excel('multilabel_data/teste-features-multiclass.xlsx', index_col=0).drop(columns=['y'])\n",
    "\n",
    "print('No preprocess')\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "print('SNV')\n",
    "print(X_snv_train.shape, X_snv_test.shape)\n",
    "\n",
    "print('Features')\n",
    "print(X_features_train.shape, X_features_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[08:14:59] ../src/metric/metric.cc:49: Unknown metric function f1\nStack trace:\n  [bt] (0) /home/flavioloss/miniconda3/envs/starfish/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x251a2d) [0x7f5b13e80a2d]\n  [bt] (1) /home/flavioloss/miniconda3/envs/starfish/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x251bf1) [0x7f5b13e80bf1]\n  [bt] (2) /home/flavioloss/miniconda3/envs/starfish/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x21566b) [0x7f5b13e4466b]\n  [bt] (3) /home/flavioloss/miniconda3/envs/starfish/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x20fa69) [0x7f5b13e3ea69]\n  [bt] (4) /home/flavioloss/miniconda3/envs/starfish/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7f5b13cd8688]\n  [bt] (5) /home/flavioloss/miniconda3/envs/starfish/lib/python3.8/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f5b6bf059dd]\n  [bt] (6) /home/flavioloss/miniconda3/envs/starfish/lib/python3.8/lib-dynload/../../libffi.so.7(+0x6067) [0x7f5b6bf05067]\n  [bt] (7) /home/flavioloss/miniconda3/envs/starfish/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x319) [0x7f5b6bf1e1e9]\n  [bt] (8) /home/flavioloss/miniconda3/envs/starfish/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x13c95) [0x7f5b6bf1ec95]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/F16776467722/Documents/trabalhos-ufes/tcc/notebooks/ga_optimization_multiclass.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bdebian/mnt/c/Users/F16776467722/Documents/trabalhos-ufes/tcc/notebooks/ga_optimization_multiclass.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bdebian/mnt/c/Users/F16776467722/Documents/trabalhos-ufes/tcc/notebooks/ga_optimization_multiclass.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmulti:softmax\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bdebian/mnt/c/Users/F16776467722/Documents/trabalhos-ufes/tcc/notebooks/ga_optimization_multiclass.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39meval_metric\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bdebian/mnt/c/Users/F16776467722/Documents/trabalhos-ufes/tcc/notebooks/ga_optimization_multiclass.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m     \u001b[39m# 'reg_lambda': 1,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bdebian/mnt/c/Users/F16776467722/Documents/trabalhos-ufes/tcc/notebooks/ga_optimization_multiclass.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bdebian/mnt/c/Users/F16776467722/Documents/trabalhos-ufes/tcc/notebooks/ga_optimization_multiclass.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m model \u001b[39m=\u001b[39m XGBClassifier(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bdebian/mnt/c/Users/F16776467722/Documents/trabalhos-ufes/tcc/notebooks/ga_optimization_multiclass.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m model, df_metrics_xgboost \u001b[39m=\u001b[39m fit_model_multiclass_no_train_test(model, \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bdebian/mnt/c/Users/F16776467722/Documents/trabalhos-ufes/tcc/notebooks/ga_optimization_multiclass.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m     X_train, X_test, y_train, y_test, \u001b[39m'\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bdebian/mnt/c/Users/F16776467722/Documents/trabalhos-ufes/tcc/notebooks/ga_optimization_multiclass.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m df_metrics_xgboost\n",
      "\u001b[1;32m/mnt/c/Users/F16776467722/Documents/trabalhos-ufes/tcc/notebooks/ga_optimization_multiclass.ipynb Cell 7\u001b[0m in \u001b[0;36mfit_model_multiclass_no_train_test\u001b[0;34m(model, X_train, X_test, y_train, y_test, method)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bdebian/mnt/c/Users/F16776467722/Documents/trabalhos-ufes/tcc/notebooks/ga_optimization_multiclass.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_model_multiclass_no_train_test\u001b[39m(model, X_train, X_test, y_train, y_test, method):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bdebian/mnt/c/Users/F16776467722/Documents/trabalhos-ufes/tcc/notebooks/ga_optimization_multiclass.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bdebian/mnt/c/Users/F16776467722/Documents/trabalhos-ufes/tcc/notebooks/ga_optimization_multiclass.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bdebian/mnt/c/Users/F16776467722/Documents/trabalhos-ufes/tcc/notebooks/ga_optimization_multiclass.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# print('ACCURACY:', round(accuracy_score(y_test, pred), 3))\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bdebian/mnt/c/Users/F16776467722/Documents/trabalhos-ufes/tcc/notebooks/ga_optimization_multiclass.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# print('BALANCED ACCURACY', round(balanced_accuracy_score(y_test, pred), 3))\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bdebian/mnt/c/Users/F16776467722/Documents/trabalhos-ufes/tcc/notebooks/ga_optimization_multiclass.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# print('F-SCORE:', round(f1_score(y_test, pred, average='macro'), 3))\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bdebian/mnt/c/Users/F16776467722/Documents/trabalhos-ufes/tcc/notebooks/ga_optimization_multiclass.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# print('PRECISION:', round(precision_score(y_test, pred, average='macro'), 3))\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bdebian/mnt/c/Users/F16776467722/Documents/trabalhos-ufes/tcc/notebooks/ga_optimization_multiclass.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# print('RECALL:', round(recall_score(y_test, pred, average='macro'), 3))\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/starfish/lib/python3.8/site-packages/xgboost/core.py:532\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    531\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 532\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/starfish/lib/python3.8/site-packages/xgboost/sklearn.py:1400\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1379\u001b[0m model, metric, params, early_stopping_rounds, callbacks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_configure_fit(\n\u001b[1;32m   1380\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1381\u001b[0m )\n\u001b[1;32m   1382\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1383\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[1;32m   1384\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1397\u001b[0m     enable_categorical\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menable_categorical,\n\u001b[1;32m   1398\u001b[0m )\n\u001b[0;32m-> 1400\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1401\u001b[0m     params,\n\u001b[1;32m   1402\u001b[0m     train_dmatrix,\n\u001b[1;32m   1403\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1404\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1405\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1406\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1407\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1408\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1409\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1410\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1411\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1412\u001b[0m )\n\u001b[1;32m   1414\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[1;32m   1415\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/starfish/lib/python3.8/site-packages/xgboost/core.py:532\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    531\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 532\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/starfish/lib/python3.8/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/starfish/lib/python3.8/site-packages/xgboost/core.py:1733\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1730\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_features(dtrain)\n\u001b[1;32m   1732\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1733\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   1734\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[1;32m   1735\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[1;32m   1736\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1737\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/starfish/lib/python3.8/site-packages/xgboost/core.py:203\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [08:14:59] ../src/metric/metric.cc:49: Unknown metric function f1\nStack trace:\n  [bt] (0) /home/flavioloss/miniconda3/envs/starfish/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x251a2d) [0x7f5b13e80a2d]\n  [bt] (1) /home/flavioloss/miniconda3/envs/starfish/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x251bf1) [0x7f5b13e80bf1]\n  [bt] (2) /home/flavioloss/miniconda3/envs/starfish/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x21566b) [0x7f5b13e4466b]\n  [bt] (3) /home/flavioloss/miniconda3/envs/starfish/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x20fa69) [0x7f5b13e3ea69]\n  [bt] (4) /home/flavioloss/miniconda3/envs/starfish/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7f5b13cd8688]\n  [bt] (5) /home/flavioloss/miniconda3/envs/starfish/lib/python3.8/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f5b6bf059dd]\n  [bt] (6) /home/flavioloss/miniconda3/envs/starfish/lib/python3.8/lib-dynload/../../libffi.so.7(+0x6067) [0x7f5b6bf05067]\n  [bt] (7) /home/flavioloss/miniconda3/envs/starfish/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x319) [0x7f5b6bf1e1e9]\n  [bt] (8) /home/flavioloss/miniconda3/envs/starfish/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x13c95) [0x7f5b6bf1ec95]\n\n"
     ]
    }
   ],
   "source": [
    "# XGBOOST\n",
    "\n",
    "def objective(trial):\n",
    "    global  X_train, X_test, y_train, y_test, X_snv_train, X_snv_test, X_features_train, X_features_test\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"eval_metric\": \"f1\",\n",
    "        'scale_pos_weight': trial.suggest_int('scale_pos_weight', 1, 25),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1),\n",
    "        'reg_alpha': trial.suggest_int('reg_alpha', 0, 20, step=1),\n",
    "        'reg_lambda': trial.suggest_int('reg_lambda', 0, 20, step=1)\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**params)\n",
    "    \n",
    "    model, metrics_df = fit_model_multiclass_no_train_test(model,\n",
    "        X_train, X_test, y_train, y_test, \"model\")\n",
    "\n",
    "\n",
    "    return metrics_df['model'].values[0]\n",
    "\n",
    "# study = optuna.create_study(sampler=TPESampler(), direction='maximize')\n",
    "# study.optimize(objective, n_trials=200)\n",
    "\n",
    "# print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "# print(\"Best trial:\")\n",
    "# trial = study.best_trial\n",
    "\n",
    "# print(\"Value: {}\".format(trial.value))\n",
    "\n",
    "# print(\"Params: \")\n",
    "# for key, value in trial.params.items():\n",
    "#     print(\"{}: {}\".format(key, value))\n",
    "\n",
    "# params = {\n",
    "#     \"objective\": \"multi:softmax\",\n",
    "#     \"eval_metric\": \"f1\",\n",
    "#     \"scale_pos_weight\": trial.params['scale_pos_weight'],\n",
    "#     \"max_depth\": trial.params['max_depth'],\n",
    "#     \"n_estimators\": trial.params['n_estimators'],\n",
    "#     \"learning_rate\": trial.params['learning_rate'],\n",
    "#     \"colsample_bytree\": trial.params['colsample_bytree'],\n",
    "#     \"subsample\": trial.params['subsample'],\n",
    "#     \"reg_alpha\": trial.params['reg_alpha'],\n",
    "#     \"reg_lambda\": trial.params['reg_lambda']\n",
    "#     }\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"eval_metric\": \"f1\",\n",
    "    # 'scale_pos_weight': 6,\n",
    "    # 'max_depth': 7,\n",
    "    # 'n_estimators': 74,\n",
    "    # 'learning_rate': 0.7081324139034884,\n",
    "    # 'colsample_bytree': 0.8084301280514788,\n",
    "    # 'subsample': 0.45732257564871703,\n",
    "    # 'reg_alpha': 9,\n",
    "    # 'reg_lambda': 1,\n",
    "}\n",
    "\n",
    "model = XGBClassifier(**params)\n",
    "\n",
    "model, df_metrics_xgboost = fit_model_multiclass_no_train_test(model, \n",
    "    X_train, X_test, y_train, y_test, 'model')\n",
    "\n",
    "df_metrics_xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-27 08:15:25,200]\u001b[0m A new study created in memory with name: no-name-cb30fa97-48c0-4b11-8ef6-a032b6319d0d\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# CATBOOST\n",
    "\n",
    "def objective(trial):\n",
    "    global  X_train, X_test, y_train, y_test, X_snv_train, X_snv_test, X_features_train, X_features_test\n",
    "\n",
    "    params = {\n",
    "        'verbose': 0,\n",
    "        'random_seed': 1066,\n",
    "        'boosting_type': trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 16), \n",
    "        'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 40),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1, 200),\n",
    "        'rsm': trial.suggest_float('rsm', 0.1, 1.0),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1)\n",
    "    }\n",
    "\n",
    "    model = CatBoostClassifier(**params)\n",
    "    \n",
    "    model, metrics_df = fit_model_multiclass_no_train_test(model,\n",
    "        X_train, X_test, y_train, y_test, \"model\")\n",
    "\n",
    "\n",
    "    return metrics_df['model'].values[0]\n",
    "\n",
    "study = optuna.create_study(sampler=TPESampler(), direction='maximize')\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"{}: {}\".format(key, value))\n",
    "\n",
    "params = {\n",
    "        'verbose': 0,\n",
    "        'random_seed': 1066,\n",
    "        'objective': 'MultiClass',\n",
    "        'boosting_type': trial.params['boosting_type'],\n",
    "        'max_depth': trial.params['max_depth'], \n",
    "        'l2_leaf_reg': trial.params['l2_leaf_reg'],\n",
    "        'n_estimators': trial.params['n_estimators'],\n",
    "        'rsm': trial.params['rsm'],\n",
    "        'learning_rate': trial.params['learning_rate']\n",
    "    }\n",
    "\n",
    "params = {\n",
    "    'verbose': 0,\n",
    "    'random_seed': 1066,\n",
    "    'objective': 'MultiClass',\n",
    "    'boosting_type': 'Ordered', \n",
    "    'max_depth': 1, \n",
    "    'l2_leaf_reg': 26, \n",
    "    'n_estimators': 158, \n",
    "    'rsm': 0.6536290864675189, \n",
    "    'learning_rate': 0.5156903366818304\n",
    "    }\n",
    "\n",
    "model = CatBoostClassifier(**params)\n",
    "\n",
    "model, df_metrics = fit_model_multiclass_no_train_test(model, \n",
    "    X_train, X_test, y_train, y_test, 'metrics')\n",
    "\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIGHTGBM\n",
    "\n",
    "def objective(trial):\n",
    "    global  X_train, X_test, y_train, y_test, X_snv_train, X_snv_test, X_features_train, X_features_test\n",
    "\n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'scale_pos_weight': trial.suggest_int('scale_pos_weight', 1, 15),\n",
    "        \"n_estimators\": trial.suggest_int('n_estimators', 10, 100),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 1),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 5000, step=20),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "    }\n",
    "\n",
    "    model = LGBMClassifier(**params)\n",
    "    \n",
    "    model, metrics_df = fit_model_multiclass_no_train_test(model,\n",
    "        X_train, X_test, y_train, y_test, \"model\")\n",
    "\n",
    "\n",
    "    return metrics_df['model'].values[0]\n",
    "\n",
    "study = optuna.create_study(sampler=TPESampler(), direction='maximize')\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"{}: {}\".format(key, value))\n",
    "\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'scale_pos_weight': trial.params['scale_pos_weight'],\n",
    "    'n_estimators': trial.params['n_estimators'],\n",
    "    'learning_rate': trial.params['learning_rate'],\n",
    "    'num_leaves': trial.params['num_leaves'],\n",
    "    'max_depth': trial.params['max_depth']\n",
    "}\n",
    "\n",
    "model = LGBMClassifier(**params)\n",
    "\n",
    "model, df_metrics = fit_model_multiclass_no_train_test(model, \n",
    "    X_train, X_test, y_train, y_test, 'df_metrics')\n",
    "\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST\n",
    "\n",
    "def objective(trial):\n",
    "    global  X_train, X_test, y_train, y_test, X_snv_train, X_snv_test, X_features_train, X_features_test\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        'scale_pos_weight': trial.suggest_int('scale_pos_weight', 1, 25),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1),\n",
    "        'reg_alpha': trial.suggest_int('reg_alpha', 0, 20, step=1),\n",
    "        'reg_lambda': trial.suggest_int('reg_lambda', 0, 20, step=1)\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**params)\n",
    "    \n",
    "    model, metrics_df = fit_model_multiclass_no_train_test(model,\n",
    "        X_snv_train, X_snv_test, y_train, y_test, \"model\")\n",
    "\n",
    "\n",
    "    return metrics_df['model'].values[0]\n",
    "\n",
    "study = optuna.create_study(sampler=TPESampler(), direction='maximize')\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"{}: {}\".format(key, value))\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"eval_metric\": \"f1\",\n",
    "    \"scale_pos_weight\": trial.params['scale_pos_weight'],\n",
    "    \"max_depth\": trial.params['max_depth'],\n",
    "    \"n_estimators\": trial.params['n_estimators'],\n",
    "    \"learning_rate\": trial.params['learning_rate'],\n",
    "    \"colsample_bytree\": trial.params['colsample_bytree'],\n",
    "    \"subsample\": trial.params['subsample'],\n",
    "    \"reg_alpha\": trial.params['reg_alpha'],\n",
    "    \"reg_lambda\": trial.params['reg_lambda']\n",
    "    }\n",
    "\n",
    "model = XGBClassifier(**params)\n",
    "\n",
    "model, df_metrics_xgboost = fit_model_multiclass_no_train_test(model, \n",
    "    X_snv_train, X_snv_test, y_train, y_test, 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATBOOST\n",
    "\n",
    "def objective(trial):\n",
    "    global  X_train, X_test, y_train, y_test, X_snv_train, X_snv_test, X_features_train, X_features_test\n",
    "\n",
    "    params = {\n",
    "        'verbose': 0,\n",
    "        'random_seed': 1066,\n",
    "        'boosting_type': trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 16), \n",
    "        'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 40),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1, 200),\n",
    "        'rsm': trial.suggest_float('rsm', 0.1, 1.0),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1)\n",
    "    }\n",
    "\n",
    "    model = CatBoostClassifier(**params)\n",
    "    \n",
    "    model, metrics_df = fit_model_multiclass_no_train_test(model,\n",
    "        X_snv_train, X_snv_test, y_train, y_test, \"model\")\n",
    "\n",
    "\n",
    "    return metrics_df['model'].values[0]\n",
    "\n",
    "study = optuna.create_study(sampler=TPESampler(), direction='maximize')\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"{}: {}\".format(key, value))\n",
    "\n",
    "params = {\n",
    "        'verbose': 0,\n",
    "        'random_seed': 1066,\n",
    "        'objective': 'MultiClass',\n",
    "        'boosting_type': trial.params['boosting_type'],\n",
    "        'max_depth': trial.params['max_depth'], \n",
    "        'l2_leaf_reg': trial.params['l2_leaf_reg'],\n",
    "        'n_estimators': trial.params['n_estimators'],\n",
    "        'rsm': trial.params['rsm'],\n",
    "        'learning_rate': trial.params['learning_rate']\n",
    "    }\n",
    "\n",
    "model = CatBoostClassifier(**params)\n",
    "\n",
    "model, df_metrics_xgboost = fit_model_multiclass_no_train_test(model, \n",
    "    X_snv_train, X_snv_test, y_train, y_test, 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIGHTGBM\n",
    "\n",
    "def objective(trial):\n",
    "    global  X_train, X_test, y_train, y_test, X_snv_train, X_snv_test, X_features_train, X_features_test\n",
    "\n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'scale_pos_weight': trial.suggest_int('scale_pos_weight', 1, 15),\n",
    "        \"n_estimators\": trial.suggest_int('n_estimators', 10, 100),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 1),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 5000, step=20),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "    }\n",
    "\n",
    "    model = LGBMClassifier(**params)\n",
    "    \n",
    "    model, metrics_df = fit_model_multiclass_no_train_test(model,\n",
    "        X_snv_train, X_snv_test, y_train, y_test, \"model\")\n",
    "\n",
    "\n",
    "    return metrics_df['model'].values[0]\n",
    "\n",
    "study = optuna.create_study(sampler=TPESampler(), direction='maximize')\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"{}: {}\".format(key, value))\n",
    "\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'scale_pos_weight': trial.params['scale_pos_weight'],\n",
    "    'n_estimators': trial.params['n_estimators'],\n",
    "    'learning_rate': trial.params['learning_rate'],\n",
    "    'num_leaves': trial.params['num_leaves'],\n",
    "    'max_depth': trial.params['max_depth']\n",
    "}\n",
    "\n",
    "model = LGBMClassifier(**params)\n",
    "\n",
    "model, df_metrics_xgboost = fit_model_multiclass_no_train_test(model, \n",
    "    X_snv_train, X_snv_test, y_train, y_test, 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST\n",
    "\n",
    "def objective(trial):\n",
    "    global  X_train, X_test, y_train, y_test, X_snv_train, X_snv_test, X_features_train, X_features_test\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        'scale_pos_weight': trial.suggest_int('scale_pos_weight', 1, 25),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 15, step=1),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1),\n",
    "        'reg_alpha': trial.suggest_int('reg_alpha', 0, 20, step=1),\n",
    "        'reg_lambda': trial.suggest_int('reg_lambda', 0, 20, step=1)\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**params)\n",
    "    \n",
    "    model, metrics_df = fit_model_multiclass_no_train_test(model,\n",
    "        X_features_train, X_features_test, y_train, y_test, \"model\")\n",
    "\n",
    "\n",
    "    return metrics_df['model'].values[0]\n",
    "\n",
    "study = optuna.create_study(sampler=TPESampler(), direction='maximize')\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"{}: {}\".format(key, value))\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"eval_metric\": \"f1\",\n",
    "    \"scale_pos_weight\": trial.params['scale_pos_weight'],\n",
    "    \"max_depth\": trial.params['max_depth'],\n",
    "    \"n_estimators\": trial.params['n_estimators'],\n",
    "    \"learning_rate\": trial.params['learning_rate'],\n",
    "    \"colsample_bytree\": trial.params['colsample_bytree'],\n",
    "    \"subsample\": trial.params['subsample'],\n",
    "    \"reg_alpha\": trial.params['reg_alpha'],\n",
    "    \"reg_lambda\": trial.params['reg_lambda']\n",
    "    }\n",
    "\n",
    "model = XGBClassifier(**params)\n",
    "\n",
    "model, df_metrics_xgboost = fit_model_multiclass_no_train_test(model, \n",
    "    X_features_train, X_features_test, y_train, y_test, 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATBOOST\n",
    "\n",
    "def objective(trial):\n",
    "    global  X_train, X_test, y_train, y_test, X_snv_train, X_snv_test, X_features_train, X_features_test\n",
    "\n",
    "    params = {\n",
    "        'verbose': 0,\n",
    "        'random_seed': 1066,\n",
    "        'boosting_type': trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 16), \n",
    "        'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 40),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1, 200),\n",
    "        'rsm': trial.suggest_float('rsm', 0.1, 1.0),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1)\n",
    "    }\n",
    "\n",
    "    model = CatBoostClassifier(**params)\n",
    "    \n",
    "    model, metrics_df = fit_model_multiclass_no_train_test(model,\n",
    "        X_features_train, X_features_test, y_train, y_test, \"model\")\n",
    "\n",
    "\n",
    "    return metrics_df['model'].values[0]\n",
    "\n",
    "study = optuna.create_study(sampler=TPESampler(), direction='maximize')\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"{}: {}\".format(key, value))\n",
    "\n",
    "params = {\n",
    "        'verbose': 0,\n",
    "        'random_seed': 1066,\n",
    "        'objective': 'MultiClass',\n",
    "        'boosting_type': trial.params['boosting_type'],\n",
    "        'max_depth': trial.params['max_depth'], \n",
    "        'l2_leaf_reg': trial.params['l2_leaf_reg'],\n",
    "        'n_estimators': trial.params['n_estimators'],\n",
    "        'rsm': trial.params['rsm'],\n",
    "        'learning_rate': trial.params['learning_rate']\n",
    "    }\n",
    "\n",
    "model = CatBoostClassifier(**params)\n",
    "\n",
    "model, df_metrics_xgboost = fit_model_multiclass_no_train_test(model, \n",
    "    X_features_train, X_features_test, y_train, y_test, 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIGHTGBM\n",
    "\n",
    "def objective(trial):\n",
    "    global  X_train, X_test, y_train, y_test, X_snv_train, X_snv_test, X_features_train, X_features_test\n",
    "\n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'scale_pos_weight': trial.suggest_int('scale_pos_weight', 1, 15),\n",
    "        \"n_estimators\": trial.suggest_int('n_estimators', 10, 100),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 1),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 5000, step=20),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "    }\n",
    "\n",
    "    model = LGBMClassifier(**params)\n",
    "    \n",
    "    model, metrics_df = fit_model_multiclass_no_train_test(model,\n",
    "        X_features_train, X_features_test, y_train, y_test, \"model\")\n",
    "\n",
    "\n",
    "    return metrics_df['model'].values[0]\n",
    "\n",
    "study = optuna.create_study(sampler=TPESampler(), direction='maximize')\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"{}: {}\".format(key, value))\n",
    "\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'scale_pos_weight': trial.params['scale_pos_weight'],\n",
    "    'n_estimators': trial.params['n_estimators'],\n",
    "    'learning_rate': trial.params['learning_rate'],\n",
    "    'num_leaves': trial.params['num_leaves'],\n",
    "    'max_depth': trial.params['max_depth']\n",
    "}\n",
    "\n",
    "model = LGBMClassifier(**params)\n",
    "\n",
    "model, df_metrics_xgboost = fit_model_multiclass_no_train_test(model, \n",
    "    X_features_train, X_features_test, y_train, y_test, 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "\n",
    "callbacks_list = [\n",
    "    # keras.callbacks.ModelCheckpoint(\n",
    "    #     filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "    #     monitor='val_loss', save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='accuracy', patience=10)\n",
    "]\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(256, 2, activation=\"relu\", input_shape=(X_features_train.shape[1], X_features_train.shape[2])))\n",
    "model.add(Conv1D(128, 2, activation=\"relu\", input_shape=(X_features_train.shape[1], X_features_train.shape[2])))\n",
    "model.add(Conv1D(64, 2, activation=\"relu\", input_shape=(X_features_train.shape[1], X_features_train.shape[2])))\n",
    "model.add(Conv1D(32, 2, activation=\"relu\", input_shape=(X_features_train.shape[1], X_features_train.shape[2])))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(6, activation = 'softmax'))\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', \n",
    "     optimizer = \"adam\",               \n",
    "              metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_features_train, y_train, batch_size=16, epochs=200, \n",
    "        #   callbacks=callbacks_list, \n",
    "          verbose=1)\n",
    "\n",
    "acc = model.evaluate(X_features_train, y_train)\n",
    "print(\"Loss:\", acc[0], \" Accuracy:\", acc[1])\n",
    "\n",
    "pred = model.predict(X_features_test)\n",
    "pred_y = pred.argmax(axis=-1)\n",
    "\n",
    "cm = confusion_matrix(y_test, pred_y)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "def eval_model_multiclass(y_test, pred, method):\n",
    "\n",
    "    # print('ACCURACY:', round(accuracy_score(y_test, pred), 3))\n",
    "    # print('BALANCED ACCURACY', round(balanced_accuracy_score(y_test, pred), 3))\n",
    "    # print('F-SCORE:', round(f1_score(y_test, pred, average='macro'), 3))\n",
    "    # print('PRECISION:', round(precision_score(y_test, pred, average='macro'), 3))\n",
    "    # print('RECALL:', round(recall_score(y_test, pred, average='macro'), 3))\n",
    "\n",
    "    df_metrics = pd.DataFrame(index=['ACCURACY', 'BALANCED ACCURACY', \n",
    "        'F-SCORE', 'PRECISION', 'RECALL'], data={method: [round(accuracy_score(y_test, pred), 3), \n",
    "                                                                    round(balanced_accuracy_score(y_test, pred), 3),\n",
    "                                                                    round(f1_score(y_test, pred, average='macro'), 3),\n",
    "                                                                    round(precision_score(y_test, pred, average='macro'), 3),\n",
    "                                                                    round(recall_score(y_test, pred, average='macro'), 3),\n",
    "                                                                    ]})\n",
    "    return df_metrics\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    global X_snv_train, X_snv_test, y_train, y_test, X_features_train, X_features_test\n",
    "    tf.random.set_seed(1066)\n",
    "\n",
    "    n_conv_layers = trial.suggest_int('n_conv_layers', 1, 3)\n",
    "    # n_deep_layers = trial.suggest_int('n_deep_layers', 1, 3)\n",
    "    model = Sequential()\n",
    "    if n_conv_layers >= 1:\n",
    "        model.add(Conv1D(trial.suggest_int('conv_neurons_1', 64, 256), \n",
    "            2, activation=\"relu\", input_shape=(X_features_train.shape[1], X_features_train.shape[2])))\n",
    "    if n_conv_layers >= 2:\n",
    "        model.add(Conv1D(trial.suggest_int('conv_neurons_2', 64, 256), \n",
    "            2, activation=\"relu\", input_shape=(X_features_train.shape[1], X_features_train.shape[2])))\n",
    "    if n_conv_layers >= 3:\n",
    "        model.add(Conv1D(trial.suggest_int('conv_neurons_3', 64, 256), \n",
    "            2, activation=\"relu\", input_shape=(X_features_train.shape[1], X_features_train.shape[2])))\n",
    "    model.add(Dense(trial.suggest_int('deep_neurons_1', 64, 256), activation=\"relu\"))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(6, activation = 'softmax'))\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', \n",
    "          optimizer=tf.keras.optimizers.Adam(\n",
    "          learning_rate=trial.suggest_categorical('learning_rate', [0.01, 0.001, 0.0001])),               \n",
    "          metrics = ['accuracy'])\n",
    "\n",
    "    model.fit(X_features_train, y_train, \n",
    "      batch_size=trial.suggest_int('batch_size', 16, 128),\n",
    "      epochs=200, \n",
    "      verbose=0)\n",
    "    \n",
    "    pred = model.predict(X_features_test)\n",
    "    pred_y = pred.argmax(axis=-1)\n",
    "\n",
    "    df_metrics_neural_net = eval_model_multiclass(y_test, pred_y, method='NeuralNet_Features')\n",
    "    \n",
    "    return df_metrics_neural_net['NeuralNet_Features'].values[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cca36271b6a067205cf30ea583e675938751abe2777c904cec6ddf3e0e3dce12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
